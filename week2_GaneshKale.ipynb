{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f46f4014-32f7-4ba7-8238-ca58ff434677",
   "metadata": {},
   "source": [
    "# Week2 - Exercise 2.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7941efc7-0c7b-4eb4-af06-c381855fb466",
   "metadata": {},
   "source": [
    "### Import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "312ed22a-755e-43e0-812c-efb835f28093",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import unicodedata\n",
    "import sys\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk import pos_tag\n",
    "from sklearn.preprocessing import LabelBinarizer, MultiLabelBinarizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e27551-16e0-4311-9a54-6210c663c2e4",
   "metadata": {},
   "source": [
    "### Load dataset into dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4586213-2a9d-4e46-b051-994db59fc13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"data/controversial-comments.jsonl\"\n",
    "\n",
    "contrv_comt = pd.read_json(filepath, lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ea4eb3-ffe0-4542-8341-c2cb4e6c70cc",
   "metadata": {},
   "source": [
    "### selecting 50K records from the contrv_comt dataset for data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a11b2cf0-74ee-45ff-bbff-e954be971eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = contrv_comt.sample(50000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3625665-706a-4efb-a019-f25ff1e92d23",
   "metadata": {},
   "source": [
    "### top 5 records of data and shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5691375e-9575-43ee-98e3-919fd4772b51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>con</th>\n",
       "      <th>txt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>170236</th>\n",
       "      <td>0</td>\n",
       "      <td>To paraphrase Bill Burr, do you think anyone g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>941856</th>\n",
       "      <td>0</td>\n",
       "      <td>didn't this guy start the hunt for WMDs, that ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211126</th>\n",
       "      <td>0</td>\n",
       "      <td>Plan A is to get his Russian mafia friends to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>733437</th>\n",
       "      <td>0</td>\n",
       "      <td>Just like the Clinton's have been wanting sinc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>474093</th>\n",
       "      <td>0</td>\n",
       "      <td>The amount of salt in this post could melt the...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        con                                                txt\n",
       "170236    0  To paraphrase Bill Burr, do you think anyone g...\n",
       "941856    0  didn't this guy start the hunt for WMDs, that ...\n",
       "211126    0  Plan A is to get his Russian mafia friends to ...\n",
       "733437    0  Just like the Clinton's have been wanting sinc...\n",
       "474093    0  The amount of salt in this post could melt the..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments.shape\n",
    "comments.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de77cea-059c-4289-89e6-4fd90cf7d81e",
   "metadata": {},
   "source": [
    "## 2.2.1. Preprocessing TEXT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1963f10-ced8-48d1-8e30-bca9a9e4a9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# created new text column where all text preprocessing actions will be performed\n",
    "\n",
    "comments['new_txt'] = comments['txt']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e88c945-a5ee-4be4-bb3b-99beff9ee964",
   "metadata": {},
   "source": [
    "A. Convert all text to lowercase letters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3275492-719c-4525-a854-de8fc086452b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['to paraphrase bill burr, do you think anyone gives $200,000 worth of a fuck to hear what she has to say? he believes that \"speaking fees\" are essentially thinly-veiled laundering of bribe money.\\n\\ni\\'d wager that if you looked into the speaking fees paid to someone and then see how they voted once they became a politician you\\'d probably see a very clear pattern in *something*.',\n",
       " \"didn't this guy start the hunt for wmds, that we never found, but we know were there, because we put them there.\",\n",
       " \"plan a is to get his russian mafia friends to steal the great wall in china and ship it here.\\n\\nfailing that, plan b is to get china to build a new one and ship it here.  and if we have to pay for it, we want a new one!  don't go making a new wall for yourselves and sending us your ratty old one.\",\n",
       " \"just like the clinton's have been wanting since the start of the campaign. he was on their list of 2-3 candidates they want to run against.\",\n",
       " 'the amount of salt in this post could melt the arctic.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# changing text column to lower case\n",
    "comments['new_txt'] = comments.new_txt.apply(lambda x : x.lower())\n",
    "\n",
    "# displaying top 5 texts of text columns as list\n",
    "comments.new_txt.head().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d064ff9-371a-4ca7-9b03-4640ceae153f",
   "metadata": {},
   "source": [
    "B. Remove all punctuation from the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c83fc81-dd17-47b2-a632-f4d0254dfc2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary of punctuations characters with keys and none as values\n",
    "\n",
    "punctuations = dict.fromkeys(i for i in range(sys.maxunicode) if unicodedata.category(chr(i)).startswith('P'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0dbbac4-83d5-4186-b30d-6dd9b98cf54b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['to paraphrase bill burr do you think anyone gives $200000 worth of a fuck to hear what she has to say he believes that speaking fees are essentially thinlyveiled laundering of bribe money\\n\\nid wager that if you looked into the speaking fees paid to someone and then see how they voted once they became a politician youd probably see a very clear pattern in something',\n",
       " 'didnt this guy start the hunt for wmds that we never found but we know were there because we put them there',\n",
       " 'plan a is to get his russian mafia friends to steal the great wall in china and ship it here\\n\\nfailing that plan b is to get china to build a new one and ship it here  and if we have to pay for it we want a new one  dont go making a new wall for yourselves and sending us your ratty old one',\n",
       " 'just like the clintons have been wanting since the start of the campaign he was on their list of 23 candidates they want to run against',\n",
       " 'the amount of salt in this post could melt the arctic']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# removing punctuations from the text column\n",
    "comments['new_txt'] = comments.new_txt.apply(lambda x : x.translate(punctuations))\n",
    "\n",
    "# displaying top 5 texts of text columns as list\n",
    "comments.new_txt.head().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad3b08b-9cba-4ee3-8039-3d205ee38749",
   "metadata": {},
   "source": [
    "C. Remove stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f79bee2-dee0-4e07-bf51-fcaf34c05490",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b6a38325-2668-4716-a4be-6923be190556",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['paraphrase bill burr think anyone gives $ 200000 worth fuck hear say believes speaking fees essentially thinlyveiled laundering bribe money id wager looked speaking fees paid someone see voted became politician youd probably see clear pattern something',\n",
       " 'didnt guy start hunt wmds never found know put',\n",
       " 'plan get russian mafia friends steal great wall china ship failing plan b get china build new one ship pay want new one dont go making new wall sending us ratty old one',\n",
       " 'like clintons wanting since start campaign list 23 candidates want run',\n",
       " 'amount salt post could melt arctic']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# removing stop words from the text column\n",
    "comments['new_txt'] = comments.new_txt.apply(lambda x : ' '.join([word for word in word_tokenize(x.lower()) if word not in stop_words]))\n",
    "\n",
    "# displaying top 5 texts of text columns as list\n",
    "comments.new_txt.head().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5cd5df7-04c8-4631-af73-83592bb7cfbc",
   "metadata": {},
   "source": [
    "D. Apply NLTK’s PorterStemmer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "82950720-8ba9-4b1d-aa14-c40e8cd356d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "porter = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9f972e20-fc77-4b9e-b96c-c6ff3f3bfd3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['paraphras bill burr think anyon give $ 200000 worth fuck hear say believ speak fee essenti thinlyveil launder bribe money id wager look speak fee paid someon see vote becam politician youd probabl see clear pattern someth',\n",
       " 'didnt guy start hunt wmd never found know put',\n",
       " 'plan get russian mafia friend steal great wall china ship fail plan b get china build new one ship pay want new one dont go make new wall send us ratti old one',\n",
       " 'like clinton want sinc start campaign list 23 candid want run',\n",
       " 'amount salt post could melt arctic']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# stemming the text columns text\n",
    "comments['new_txt'] = comments.new_txt.apply(lambda x : ' '.join([porter.stem(word) for word in word_tokenize(x)]))\n",
    "\n",
    "# displaying top 5 texts of text columns as list\n",
    "comments.new_txt.head().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea92b41-cf51-4edc-b6f6-91bc280fe3ec",
   "metadata": {},
   "source": [
    "## 2.2.2. Now that the data is pre-processed, you will apply three different techniques to get it into a usable form for model-building. Apply each of the following steps (individually) to the pre-processed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e608318a-4989-440c-a566-5361ee88b90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list of all texts from new_text column\n",
    "\n",
    "texts = comments.new_txt.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129681b4-1857-4ad7-a321-59186b0f09f3",
   "metadata": {},
   "source": [
    "A. Convert each text entry into a word-count vector (see sections 5.3 & 6.8 in the Machine Learning with Python Cookbook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d33787f7-f781-47a3-a4cf-32784d09674b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating instance of Count vectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2ceaa07f-794f-4add-b445-5e0f3430288c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<50000x32261 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 810482 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tramsforming the texts data into bag of words\n",
    "\n",
    "word_count_vector =  vectorizer.fit_transform(texts)\n",
    "word_count_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fe69a1ff-ca49-4ed5-9d71-e7cb89eee4a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display top 4 items of word vector\n",
    "\n",
    "word_count_vector.toarray()[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2f194c-1d4b-4324-b10d-a6905a2c9e51",
   "metadata": {},
   "source": [
    "B. Convert each text entry into a part-of-speech tag vector (see section 6.7 in the Machine Learning with Python Cookbook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4ec52167-4444-4598-93e8-d30724571ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add pos tagged text into list for each text of comments new_text column\n",
    "\n",
    "pos_tagged_texts = []\n",
    "post_text_tags = []\n",
    "for text in texts:\n",
    "    tag_text = pos_tag(word_tokenize(text))\n",
    "    pos_tagged_texts.append(tag_text)\n",
    "    post_text_tags.append([tag for word, tag in tag_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1c61683a-62c6-46b3-9c96-34b05ee9c9dd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('didnt', 'NN'),\n",
       " ('guy', 'JJ'),\n",
       " ('start', 'NN'),\n",
       " ('hunt', 'NN'),\n",
       " ('wmd', 'NN'),\n",
       " ('never', 'RB'),\n",
       " ('found', 'VBN'),\n",
       " ('know', 'VBP'),\n",
       " ('put', 'VB')]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['NN', 'JJ', 'NN', 'NN', 'NN', 'RB', 'VBN', 'VBP', 'VB']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pos tagged text list  and tags lists few samples - \n",
    "\n",
    "pos_tagged_texts[1]\n",
    "post_text_tags[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "47054531-7e82-4791-9af9-93aba1ec756a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using one-hot encoding to convert tags into pos tag vectors\n",
    "\n",
    "\n",
    "one_hot_multi = MultiLabelBinarizer()\n",
    "pos_vector = one_hot_multi.fit_transform(post_text_tags)\n",
    "pos_vector[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f40f875-5ce9-4527-8b68-9fec48be9f82",
   "metadata": {},
   "source": [
    "C. Convert each entry into a term frequency-inverse document frequency (tfidf) vector (see section 6.9 in the Machine Learning with Python Cookbook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "243a8754-2fb8-46c4-99ee-6fb04c56e64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create instance of tf-idf\n",
    "\n",
    "tfidf = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "656ade87-5bff-4004-8180-630561cedef1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<50000x32261 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 810482 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidef_vector = tfidf.fit_transform(texts)\n",
    "tfidef_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fac2f3a8-8d83-404b-8dd6-5ec01cb7e44a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display top 4 records of tf-idf vector\n",
    "\n",
    "tfidef_vector.toarray()[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ab8855-c2fe-442c-b696-6f91ac78601a",
   "metadata": {},
   "source": [
    "For the three techniques in problem (2) above, give an example where each would be useful."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2cd440-f774-4390-960c-20c7046b1f14",
   "metadata": {},
   "source": [
    "1. Bag of words - It is orderless document representation, and count of words only matters. This method is often used for document classification and sentiment analysis etc.\n",
    "\n",
    "2. Part of Speech - In this method we tag each word to its part of speech and this used in NLP applications such as Named Entity Recognition, Question Answering and sentiment analysis etc.\n",
    "\n",
    "3. TFIDF -  it is often used to see weightage of words in documents to learn how important that word is and used for information retrieval and text mining. The main use of this is in search engines where it can be used to determine the importance of searched word."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e58bf08-680e-4b15-8695-f0ae25dddb0f",
   "metadata": {},
   "source": [
    "# END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
